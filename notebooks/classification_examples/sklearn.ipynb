{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Classification with Delira and SciKit-Learn - A very short introduction\n",
        "*Author: Justus Schock* \n",
        "\n",
        "*Date: 31.07.2019*\n",
        "\n",
        "This Example shows how to set up a basic classification model and experiment using SciKit-Learn.\n",
        "\n",
        "Let\u0027s first setup the essential hyperparameters. We will use `delira`\u0027s `Parameters`-class for this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\pywt\\_utils.py:6: DeprecationWarning: Using or importing the ABCs from \u0027collections\u0027 instead of from \u0027collections.abc\u0027 is deprecated, and in 3.8 it will stop working\n",
            "  from collections import Iterable\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\google\\protobuf\\descriptor.py:47: DeprecationWarning: Using or importing the ABCs from \u0027collections\u0027 instead of from \u0027collections.abc\u0027 is deprecated, and in 3.8 it will stop working\n",
            "  from google.protobuf.pyext import _message\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:1286: DeprecationWarning: Using or importing the ABCs from \u0027collections\u0027 instead of from \u0027collections.abc\u0027 is deprecated, and in 3.8 it will stop working\n",
            "  _pywrap_tensorflow.RegisterType(\"Mapping\", _collections.Mapping)\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:1287: DeprecationWarning: Using or importing the ABCs from \u0027collections\u0027 instead of from \u0027collections.abc\u0027 is deprecated, and in 3.8 it will stop working\n",
            "  _pywrap_tensorflow.RegisterType(\"Sequence\", _collections.Sequence)\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_qint8 \u003d np.dtype([(\"qint8\", np.int8, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_quint8 \u003d np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_qint16 \u003d np.dtype([(\"qint16\", np.int16, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_quint16 \u003d np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_qint32 \u003d np.dtype([(\"qint32\", np.int32, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  np_resource \u003d np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\object_identity.py:61: DeprecationWarning: Using or importing the ABCs from \u0027collections\u0027 instead of from \u0027collections.abc\u0027 is deprecated, and in 3.8 it will stop working\n",
            "  class ObjectIdentityDictionary(collections.MutableMapping):\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\object_identity.py:112: DeprecationWarning: Using or importing the ABCs from \u0027collections\u0027 instead of from \u0027collections.abc\u0027 is deprecated, and in 3.8 it will stop working\n",
            "  class ObjectIdentitySet(collections.MutableSet):\n"
          ]
        }
      ],
      "source": [
        "logger \u003d None\n",
        "from delira.training import Parameters\n",
        "import sklearn\n",
        "params \u003d Parameters(fixed_params\u003d{\n",
        "    \"model\": {},\n",
        "    \"training\": {\n",
        "        \"batch_size\": 64, # batchsize to use\n",
        "        \"num_epochs\": 10, # number of epochs to train\n",
        "        \"optimizer_cls\": None, # optimization algorithm to use\n",
        "        \"optimizer_params\": {}, # initialization parameters for this algorithm\n",
        "        \"losses\": {}, # the loss function\n",
        "        \"lr_sched_cls\": None,  # the learning rate scheduling algorithm to use\n",
        "        \"lr_sched_params\": {}, # the corresponding initialization parameters\n",
        "        \"metrics\": {\"mae\": mean_absolute_error} # and some evaluation metrics\n",
        "    }\n",
        "}) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Since we did not specify any metric, only the `CrossEntropyLoss` will be calculated for each batch. Since we have a classification task, this should be sufficient. We will train our network with a batchsize of 64 by using `Adam` as optimizer of choice.\n",
        "\n",
        "## Logging and Visualization\n",
        "To get a visualization of our results, we should monitor them somehow. For logging we will use `Tensorboard`. Per default the logging directory will be the same as our experiment directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\n",
        "## Data Preparation\n",
        "### Loading\n",
        "Next we will create some fake data. For this we use the `ClassificationFakeData`-Dataset, which is already implemented in `deliravision`. To avoid getting the exact same data from both datasets, we use a random offset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "from deliravision.data.fakedata import ClassificationFakeData\n",
        "dataset_train \u003d ClassificationFakeData(num_samples\u003d10000, \n",
        "                                       img_size\u003d(3, 32, 32), \n",
        "                                       num_classes\u003d10)\n",
        "dataset_val \u003d ClassificationFakeData(num_samples\u003d1000, \n",
        "                                     img_size\u003d(3, 32, 32), \n",
        "                                     num_classes\u003d10,\n",
        "                                     rng_offset\u003d10001\n",
        "                                     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Augmentation\n",
        "For Data-Augmentation we will apply a few transformations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "from batchgenerators.transforms import RandomCropTransform, \\\n                                        ContrastAugmentationTransform, Compose\nfrom batchgenerators.transforms.spatial_transforms import ResizeTransform\nfrom batchgenerators.transforms.sample_normalization_transforms import MeanStdNormalizationTransform\n\ntransforms \u003d Compose([\n    RandomCropTransform(24), # Perform Random Crops of Size 24 x 24 pixels\n    ResizeTransform(32), # Resample these crops back to 32 x 32 pixels\n    ContrastAugmentationTransform(), # randomly adjust contrast\n    MeanStdNormalizationTransform(mean\u003d[0.5], std\u003d[0.5])]) \n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "With these transformations we can now wrap our datasets into datamanagers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "from delira.data_loading import DataManager, SequentialSampler, RandomSampler\n",
        "\n",
        "manager_train \u003d DataManager(dataset_train, params.nested_get(\"batch_size\"),\n",
        "                                transforms\u003dtransforms,\n",
        "                                sampler_cls\u003dRandomSampler,\n",
        "                                n_process_augmentation\u003d4)\n",
        "\n",
        "manager_val \u003d DataManager(dataset_val, params.nested_get(\"batch_size\"),\n",
        "                              transforms\u003dtransforms,\n",
        "                              sampler_cls\u003dSequentialSampler,\n",
        "                              n_process_augmentation\u003d4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Model\n",
        "\n",
        "After we have done that, we can specify our model: We will use a very simple MultiLayer Perceptron here. \n",
        "In opposite to other backends, we don\u0027t need to provide a custom implementation of our model, but we can simply use it as-is. It will be automatically wrapped by `SklearnEstimator`, which can be subclassed for more advanced usage.\n",
        "\n",
        "## Training\n",
        "Now that we have defined our network, we can finally specify our experiment and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning) # ignore UserWarnings raised by dependency code\n",
        "warnings.simplefilter(\"ignore\", FutureWarning) # ignore FutureWarnings raised by dependency code\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from delira.training import SklearnExperiment\n",
        "\n",
        "if logger is not None:\n",
        "    logger.info(\"Init Experiment\")\n",
        "experiment \u003d PyTorchExperiment(params, MLPClassifier,\n",
        "                               name\u003d\"ClassificationExample\",\n",
        "                               save_path\u003d\"./tmp/delira_Experiments\",\n",
        "                               key_mapping\u003d{\"X\": \"X\"}\n",
        "                               gpu_ids\u003d[0])\n",
        "experiment.save()\n",
        "\n",
        "model \u003d experiment.run(manager_train, manager_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Congratulations, you have now trained your first Classification Model using `delira`, we will now predict a few samples from the testset to show, that the networks predictions are valid (for now, this is done manually, but we also have a `Predictor` class to automate stuff like this):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm # utility for progress bars\n",
        "\n",
        "preds, labels \u003d [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(len(dataset_val))):\n",
        "        img \u003d dataset_val[i][\"data\"] # get image from current batch\n",
        "        img_tensor \u003d img.astype(np.float) # create a tensor from image, push it to device and add batch dimension\n",
        "        pred_tensor \u003d model(img_tensor) # feed it through the network\n",
        "        pred \u003d pred_tensor.argmax(1).item() # get index with maximum class confidence\n",
        "        label \u003d np.asscalar(dataset_val[i][\"label\"]) # get label from batch\n",
        "        if i % 1000 \u003d\u003d 0:\n",
        "            print(\"Prediction: %d \\t label: %d\" % (pred, label)) # print result\n",
        "        preds.append(pred)\n",
        "        labels.append(label)\n",
        "        \n",
        "# calculate accuracy\n",
        "accuracy \u003d (np.asarray(preds) \u003d\u003d np.asarray(labels)).sum() / len(preds)\n",
        "print(\"Accuracy: %.3f\" % accuracy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}