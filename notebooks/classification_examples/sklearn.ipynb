{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Classification with Delira and SciKit-Learn - A very short introduction\n",
    "*Author: Justus Schock* \n",
    "\n",
    "*Date: 31.07.2019*\n",
    "\n",
    "This Example shows how to set up a basic classification model and experiment using SciKit-Learn.\n",
    "\n",
    "Let's first setup the essential hyperparameters. We will use `delira`'s `Parameters`-class for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\pywt\\_utils.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\google\\protobuf\\descriptor.py:47: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from google.protobuf.pyext import _message\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  _pywrap_tensorflow.RegisterType(\"Mapping\", _collections.Mapping)\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:1287: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  _pywrap_tensorflow.RegisterType(\"Sequence\", _collections.Sequence)\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class ObjectIdentityDictionary(collections.MutableMapping):\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\object_identity.py:112: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class ObjectIdentitySet(collections.MutableSet):\n"
     ]
    }
   ],
   "source": [
    "logger = None\n",
    "from delira.training import Parameters\n",
    "import sklearn\n",
    "params = Parameters(fixed_params={\n",
    "    \"model\": {},\n",
    "    \"training\": {\n",
    "        \"batch_size\": 64, # batchsize to use\n",
    "        \"num_epochs\": 10, # number of epochs to train\n",
    "        \"optimizer_cls\": None, # optimization algorithm to use\n",
    "        \"optimizer_params\": {}, # initialization parameters for this algorithm\n",
    "        \"losses\": {}, # the loss function\n",
    "        \"lr_sched_cls\": None,  # the learning rate scheduling algorithm to use\n",
    "        \"lr_sched_params\": {}, # the corresponding initialization parameters\n",
    "        \"metrics\": {\"mae\": mean_absolute_error} # and some evaluation metrics\n",
    "    }\n",
    "}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Since we did not specify any metric, only the `CrossEntropyLoss` will be calculated for each batch. Since we have a classification task, this should be sufficient. We will train our network with a batchsize of 64 by using `Adam` as optimizer of choice.\n",
    "\n",
    "## Logging and Visualization\n",
    "To get a visualization of our results, we should monitor them somehow. For logging we will use `Tensorboard`. Per default the logging directory will be the same as our experiment directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "## Data Preparation\n",
    "### Loading\n",
    "Next we will create some fake data. For this we use the `ClassificationFakeData`-Dataset, which is already implemented in `deliravision`. To avoid getting the exact same data from both datasets, we use a random offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from deliravision.data.fakedata import ClassificationFakeData\n",
    "dataset_train = ClassificationFakeData(num_samples=10000, \n",
    "                                       img_size=(3, 32, 32), \n",
    "                                       num_classes=10)\n",
    "dataset_val = ClassificationFakeData(num_samples=1000, \n",
    "                                     img_size=(3, 32, 32), \n",
    "                                     num_classes=10,\n",
    "                                     rng_offset=10001\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Augmentation\n",
    "For Data-Augmentation we will apply a few transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from batchgenerators.transforms import RandomCropTransform, \\\n",
    "                                        ContrastAugmentationTransform, Compose\n",
    "from batchgenerators.transforms.spatial_transforms import ResizeTransform\n",
    "from batchgenerators.transforms.sample_normalization_transforms import MeanStdNormalizationTransform\n",
    "\n",
    "transforms = Compose([\n",
    "    RandomCropTransform(24), # Perform Random Crops of Size 200 x 200 pixels\n",
    "    ResizeTransform(32), # Resample these crops back to 224 x 224 pixels\n",
    "    ContrastAugmentationTransform(), # randomly adjust contrast\n",
    "    MeanStdNormalizationTransform(mean=[0.5], std=[0.5])]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "With these transformations we can now wrap our datasets into datamanagers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from delira.data_loading import BaseDataManager, SequentialSampler, RandomSampler\n",
    "\n",
    "manager_train = BaseDataManager(dataset_train, params.nested_get(\"batch_size\"),\n",
    "                                transforms=transforms,\n",
    "                                sampler_cls=RandomSampler,\n",
    "                                n_process_augmentation=4)\n",
    "\n",
    "manager_val = BaseDataManager(dataset_val, params.nested_get(\"batch_size\"),\n",
    "                              transforms=transforms,\n",
    "                              sampler_cls=SequentialSampler,\n",
    "                              n_process_augmentation=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Model\n",
    "\n",
    "After we have done that, we can specify our model: We will use a very simple MultiLayer Perceptron here. \n",
    "In opposite to other backends, we don't need to provide a custom implementation of our model, but we can simply use it as-is. It will be automatically wrapped by `SklearnEstimator`, which can be subclassed for more advanced usage.\n",
    "\n",
    "## Training\n",
    "Now that we have defined our network, we can finally specify our experiment and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning) # ignore UserWarnings raised by dependency code\n",
    "warnings.simplefilter(\"ignore\", FutureWarning) # ignore FutureWarnings raised by dependency code\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from delira.training import SklearnExperiment\n",
    "\n",
    "if logger is not None:\n",
    "    logger.info(\"Init Experiment\")\n",
    "experiment = PyTorchExperiment(params, MLPClassifier,\n",
    "                               name=\"ClassificationExample\",\n",
    "                               save_path=\"./tmp/delira_Experiments\",\n",
    "                               key_mapping={\"X\": \"X\"}\n",
    "                               gpu_ids=[0])\n",
    "experiment.save()\n",
    "\n",
    "model = experiment.run(manager_train, manager_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Congratulations, you have now trained your first Classification Model using `delira`, we will now predict a few samples from the testset to show, that the networks predictions are valid (for now, this is done manually, but we also have a `Predictor` class to automate stuff like this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm # utility for progress bars\n",
    "\n",
    "preds, labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(dataset_val))):\n",
    "        img = dataset_val[i][\"data\"] # get image from current batch\n",
    "        img_tensor = img.astype(np.float) # create a tensor from image, push it to device and add batch dimension\n",
    "        pred_tensor = model(img_tensor) # feed it through the network\n",
    "        pred = pred_tensor.argmax(1).item() # get index with maximum class confidence\n",
    "        label = np.asscalar(dataset_val[i][\"label\"]) # get label from batch\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Prediction: %d \\t label: %d\" % (pred, label)) # print result\n",
    "        preds.append(pred)\n",
    "        labels.append(label)\n",
    "        \n",
    "# calculate accuracy\n",
    "accuracy = (np.asarray(preds) == np.asarray(labels)).sum() / len(preds)\n",
    "print(\"Accuracy: %.3f\" % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
