{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Classification with Delira and TensorFlow Graph Execution- A very short introduction\n",
        "*Author: Justus Schock* \n",
        "\n",
        "*Date: 31.07.2019*\n",
        "\n",
        "This Example shows how to set up a basic classification model and experiment using TensorFlow\u0027s Graph Execution Mode.\n",
        "\n",
        "Let\u0027s first setup the essential hyperparameters. We will use `delira`\u0027s `Parameters`-class for this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_qint8 \u003d np.dtype([(\"qint8\", np.int8, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_quint8 \u003d np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_qint16 \u003d np.dtype([(\"qint16\", np.int16, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_quint16 \u003d np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_qint32 \u003d np.dtype([(\"qint32\", np.int32, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  np_resource \u003d np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_qint8 \u003d np.dtype([(\"qint8\", np.int8, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_quint8 \u003d np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_qint16 \u003d np.dtype([(\"qint16\", np.int16, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_quint16 \u003d np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  _np_qint32 \u003d np.dtype([(\"qint32\", np.int32, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or \u00271type\u0027 as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0027(1,)type\u0027.\n",
            "  np_resource \u003d np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\pywt\\_utils.py:6: DeprecationWarning: Using or importing the ABCs from \u0027collections\u0027 instead of from \u0027collections.abc\u0027 is deprecated, and in 3.8 it will stop working\n",
            "  from collections import Iterable\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0731 13:38:30.713174 21496 deprecation_wrapper.py:119] From c:\\users\\jsc7rng\\downloads\\delira\\delira\\models\\backends\\tf_eager\\abstract_network.py:113: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0731 13:38:30.727135 21496 deprecation_wrapper.py:119] From c:\\users\\jsc7rng\\downloads\\delira\\delira\\models\\backends\\tf_graph\\abstract_network.py:20: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "logger \u003d None\n",
        "import tensorflow as tf\n",
        "tf.disable_eager_execution()\n",
        "from delira.training import Parameters\n",
        "params \u003d Parameters(fixed_params\u003d{\n",
        "    \"model\": {\n",
        "        \"in_channels\": 1, \n",
        "        \"n_outputs\": 10\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"batch_size\": 64, # batchsize to use\n",
        "        \"num_epochs\": 10, # number of epochs to train\n",
        "        \"optimizer_cls\": tf.train.AdamOptimizer, # optimization algorithm to use\n",
        "        \"optimizer_params\": {\u0027lr\u0027: 1e-3}, # initialization parameters for this algorithm\n",
        "        \"losses\": {\"L1\": tf.losses.absolute_difference}, # the loss function\n",
        "        \"lr_sched_cls\": None,  # the learning rate scheduling algorithm to use\n",
        "        \"lr_sched_params\": {}, # the corresponding initialization parameters\n",
        "        \"metrics\": {} # and some evaluation metrics\n",
        "    }\n",
        "}) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Since we did not specify any metric, only the `L1-Loss` will be calculated for each batch. Since this is just a toy example, this should be sufficient. We will train our network with a batchsize of 64 by using `Adam` as optimizer of choice.\n",
        "\n",
        "## Logging and Visualization\n",
        "To get a visualization of our results, we should monitor them somehow. For logging we will use `Tensorboard`. Per default the logging directory will be the same as our experiment directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\n",
        "## Data Preparation\n",
        "### Loading\n",
        "Next we will create some fake data. For this we use the `ClassificationFakeData`-Dataset, which is already implemented in `deliravision`. To avoid getting the exact same data from both datasets, we use a random offset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named \u0027deliravision\u0027",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-2-c638229a3dc2\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[1;32m----\u003e 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdeliravision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfakedata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClassificationFakeData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m dataset_train \u003d ClassificationFakeData(num_samples\u003d10000, \n\u001b[0;32m      3\u001b[0m                                        \u001b[0mimg_size\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                        num_classes\u003d10)\n\u001b[0;32m      5\u001b[0m dataset_val \u003d ClassificationFakeData(num_samples\u003d1000, \n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named \u0027deliravision\u0027"
          ]
        }
      ],
      "source": [
        "from deliravision.data.fakedata import ClassificationFakeData\n",
        "dataset_train \u003d ClassificationFakeData(num_samples\u003d10000, \n",
        "                                       img_size\u003d(3, 32, 32), \n",
        "                                       num_classes\u003d10)\n",
        "dataset_val \u003d ClassificationFakeData(num_samples\u003d1000, \n",
        "                                     img_size\u003d(3, 32, 32), \n",
        "                                     num_classes\u003d10,\n",
        "                                     rng_offset\u003d10001\n",
        "                                     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Augmentation\n",
        "For Data-Augmentation we will apply a few transformations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "from batchgenerators.transforms import RandomCropTransform, \\\n                                        ContrastAugmentationTransform, Compose\nfrom batchgenerators.transforms.spatial_transforms import ResizeTransform\nfrom batchgenerators.transforms.sample_normalization_transforms import MeanStdNormalizationTransform\n\ntransforms \u003d Compose([\n    RandomCropTransform(24), # Perform Random Crops of Size 24 x 24 pixels\n    ResizeTransform(32), # Resample these crops back to 32 x 32 pixels\n    ContrastAugmentationTransform(), # randomly adjust contrast\n    MeanStdNormalizationTransform(mean\u003d[0.5], std\u003d[0.5])]) \n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "With these transformations we can now wrap our datasets into datamanagers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "from delira.data_loading import BaseDataManager, SequentialSampler, RandomSampler\n",
        "\n",
        "manager_train \u003d BaseDataManager(dataset_train, params.nested_get(\"batch_size\"),\n",
        "                                transforms\u003dtransforms,\n",
        "                                sampler_cls\u003dRandomSampler,\n",
        "                                n_process_augmentation\u003d4)\n",
        "\n",
        "manager_val \u003d BaseDataManager(dataset_val, params.nested_get(\"batch_size\"),\n",
        "                              transforms\u003dtransforms,\n",
        "                              sampler_cls\u003dSequentialSampler,\n",
        "                              n_process_augmentation\u003d4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Model\n",
        "\n",
        "After we have done that, we can specify our model: We will use a smaller version of a [VGG-Network](https://arxiv.org/pdf/1409.1556.pdf) in this case. We will use more convolutions to reduce the feature dimensionality and reduce the number of units in the linear layers to save up memory (and we only have to deal with 10 classes, not the 1000 imagenet classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from delira.models import AbstractTfGraphNetwork\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class SmallVGGTfEager(AbstractTfGraphNetwork):\n",
        "    def __init__(self, in_channels, num_classes, data_format\u003d\"channels_last\"):\n",
        "        if data_format \u003d\u003d \"channels_last\":\n",
        "            input_shape \u003d (32, 32, 3)\n",
        "        else:\n",
        "            input_shape \u003d (3, 32, 32)\n",
        "        super().__init__()\n",
        "        \n",
        "        self.model \u003d tf.keras.models.Sequential(\n",
        "            tf.keras.layers.Conv2d(in_channels, 64, 3, padding\u003d1, input_shape\u003dinput_shape), # 32, 32\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPool2d(2), # 16 x 16\n",
        "            tf.keras.layers.Conv2d(128, 3, padding\u003d1),\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPool2d(2), # 8 x 8\n",
        "            tf.keras.layers.Conv2d(256, 3, padding\u003d1),\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPool2d(2), # 4 x 4\n",
        "            tf.keras.layers.Conv2d(512, 3, padding\u003d1),\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPool2d(), # 2 x 2\n",
        "            tf.keras.layers.Conv2d(512, 3, padding\u003d1),\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPool2d(), # 1 x 1\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(num_classes),\n",
        "        )\n",
        "        \n",
        "        # create computation graph\n",
        "        data \u003d tf.placeholder(shape\u003d[None, 32], dtype\u003dtf.float32)\n",
        "        labels \u003d tf.placeholder_with_default(\n",
        "                tf.zeros([tf.shape(data)[0], 1]), shape\u003d[None, 1])\n",
        "\n",
        "        preds_train \u003d self.model(data)\n",
        "        preds_eval \u003d self.model(data)\n",
        "\n",
        "        self.inputs[\"data\"] \u003d data\n",
        "        self.inputs[\"label\"] \u003d labels\n",
        "        self.outputs_train[\"pred\"] \u003d preds_train\n",
        "        self.outputs_eval[\"pred\"] \u003d preds_eval\n",
        "        \n",
        "    @staticmethod\n",
        "    def prepare_batch(data_dict, input_device, output_device):\n",
        "        with tf.device(input_device):\n",
        "            return_dict \u003d {\"data\": tf.convert.to.tensor(\n",
        "                batch[\"data\"].astype(np.float32))}\n",
        "        \n",
        "        with tf.device(output_device):\n",
        "            for key, vals in batch.items():\n",
        "                if key \u003d\u003d \"data\": \n",
        "                    continue\n",
        "                return_dict[key] \u003d tf.convert_to_tensor(\n",
        "                    vals.astype(np.float32))\n",
        "\n",
        "        return return_dict\n",
        "    \n",
        "    @staticmethod\n",
        "    def closure(model, data_dict: dict, optimizers: dict, losses: dict,\n",
        "                fold\u003d0, **kwargs):\n",
        "\n",
        "        outputs \u003d model.run(data\u003dinputs, label\u003ddata_dict[\u0027label\u0027])\n",
        "        preds \u003d outputs[\u0027pred\u0027]\n",
        "        loss_vals \u003d outputs[\u0027losses\u0027]\n",
        "        \n",
        "        return loss_vals, preds\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "So let\u0027s evisit, what we have just done.\n",
        "\n",
        "In `delira` all networks must be derived from `delira.models.AbstractNetwork`. For each backend there is a class derived from this class, handling some backend-specific function calls and registrations. For the `Tensorflow Graph` Backend this class is `AbstractTfGraphNetwork` and all TensorFlow Eager Execution Networks should be derived from it.\n",
        "\n",
        "First we defined the network itself (this is the part simply concatenating the layers into a sequential model). Next, we defined the logic to apply, when we want to predict from the model (this is the `call` method).\n",
        "\n",
        "So far this was plain `TensorFlow`. The `prepare_batch` function is not plain TF anymore, but allows us to ensure the data is in the correct shape, has the correct data-type and lies on the correct device. The function above is the standard `prepare_batch` function, which is also implemented in the `AbstractTfGraphNetwork` and just re-implemented here for the sake of completeness.\n",
        "\n",
        "Same goes for the `closure` function. This function defines the update rule for our parameters (and how to calculate the losses). These funcitons are good to go for many simple networks but can be overwritten for customization when training more complex networks.\n",
        "\n",
        "\n",
        "## Training\n",
        "Now that we have defined our network, we can finally specify our experiment and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning) # ignore UserWarnings raised by dependency code\n",
        "warnings.simplefilter(\"ignore\", FutureWarning) # ignore FutureWarnings raised by dependency code\n",
        "\n",
        "\n",
        "from delira.training import TfGraphExperiment\n",
        "\n",
        "if logger is not None:\n",
        "    logger.info(\"Init Experiment\")\n",
        "experiment \u003d TfGraphExperiment(params, SmallVGGTfGraph,\n",
        "                               name\u003d\"ClassificationExample\",\n",
        "                               save_path\u003d\"./tmp/delira_Experiments\",\n",
        "                               key_mapping\u003d{\"x\": \"data\"}\n",
        "                               gpu_ids\u003d[0])\n",
        "experiment.save()\n",
        "\n",
        "model \u003d experiment.run(manager_train, manager_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Congratulations, you have now trained your first Classification Model using `delira`, we will now predict a few samples from the testset to show, that the networks predictions are valid (for now, this is done manually, but we also have a `Predictor` class to automate stuff like this):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm # utility for progress bars\n",
        "import tensorflow as tf\n",
        "\n",
        "device \u003d \"/cpu:0\"\n",
        "preds, labels \u003d [], []\n",
        "\n",
        "with tf.device(device):\n",
        "    for i in tqdm(range(len(dataset_val))):\n",
        "        img \u003d dataset_val[i][\"data\"] # get image from current batch\n",
        "        img_tensor \u003d tf.convert_to_tensor(img[None, ...].astype(np.float)) # create a tensor from image, push it to device and add batch dimension\n",
        "        pred_tensor \u003d model(img_tensor) # feed it through the network\n",
        "        pred \u003d pred_tensor.argmax(1).item() # get index with maximum class confidence\n",
        "        label \u003d np.asscalar(dataset_val[i][\"label\"]) # get label from batch\n",
        "        if i % 1000 \u003d\u003d 0:\n",
        "            print(\"Prediction: %d \\t label: %d\" % (pred, label)) # print result\n",
        "        preds.append(pred)\n",
        "        labels.append(label)\n",
        "\n",
        "# calculate accuracy\n",
        "accuracy \u003d (np.asarray(preds) \u003d\u003d np.asarray(labels)).sum() / len(preds)\n",
        "print(\"Accuracy: %.3f\" % accuracy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}