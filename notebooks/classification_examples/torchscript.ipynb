{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Classification with Delira and TorchScript - A very short introduction\n",
        "*Author: Justus Schock* \n",
        "\n",
        "*Date: 04.12.2018*\n",
        "\n",
        "This Example shows how to set up a basic classification `TorchScript` model and experiment.\n",
        "`TorchScript` is basically `PyTorch` with a static computation graph. Thus, we require only minor changes compared to the `PyTorch`-example. These changes will be highlighted.\n",
        "\n",
        "Let\u0027s first setup the essential hyperparameters. We will use `delira`\u0027s `Parameters`-class for this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "logger \u003d None\n",
        "import torch\n",
        "from delira.training import Parameters\n",
        "params \u003d Parameters(fixed_params\u003d{\n",
        "    \"model\": {\n",
        "        \"in_channels\": 1, \n",
        "        \"n_outputs\": 10\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"batch_size\": 64, # batchsize to use\n",
        "        \"num_epochs\": 10, # number of epochs to train\n",
        "        \"optimizer_cls\": torch.optim.Adam, # optimization algorithm to use\n",
        "        \"optimizer_params\": {\u0027lr\u0027: 1e-3}, # initialization parameters for this algorithm\n",
        "        \"losses\": {\"CE\": torch.nn.CrossEntropyLoss()}, # the loss function\n",
        "        \"lr_sched_cls\": None,  # the learning rate scheduling algorithm to use\n",
        "        \"lr_sched_params\": {}, # the corresponding initialization parameters\n",
        "        \"metrics\": {} # and some evaluation metrics\n",
        "    }\n",
        "}) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Since we did not specify any metric, only the `CrossEntropyLoss` will be calculated for each batch. Since we have a classification task, this should be sufficient. We will train our network with a batchsize of 64 by using `Adam` as optimizer of choice.\n",
        "\n",
        "## Logging and Visualization\n",
        "To get a visualization of our results, we should monitor them somehow. For logging we will use `Tensorboard`. Per default the logging directory will be the same as our experiment directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\n",
        "## Data Preparation\n",
        "### Loading\n",
        "Next we will create some fake data. For this we use the `ClassificationFakeData`-Dataset, which is already implemented in `deliravision`. To avoid getting the exact same data from both datasets, we use a random offset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "from deliravision.data.fakedata import ClassificationFakeData\n",
        "dataset_train \u003d ClassificationFakeData(num_samples\u003d10000, \n",
        "                                       img_size\u003d(3, 32, 32), \n",
        "                                       num_classes\u003d10)\n",
        "dataset_val \u003d ClassificationFakeData(num_samples\u003d1000, \n",
        "                                     img_size\u003d(3, 32, 32), \n",
        "                                     num_classes\u003d10,\n",
        "                                     rng_offset\u003d10001\n",
        "                                     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Augmentation\n",
        "For Data-Augmentation we will apply a few transformations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "from batchgenerators.transforms import RandomCropTransform, \\\n                                        ContrastAugmentationTransform, Compose\nfrom batchgenerators.transforms.spatial_transforms import ResizeTransform\nfrom batchgenerators.transforms.sample_normalization_transforms import MeanStdNormalizationTransform\n\ntransforms \u003d Compose([\n    RandomCropTransform(24), # Perform Random Crops of Size 24 x 24 pixels\n    ResizeTransform(32), # Resample these crops back to 32 x 32 pixels\n    ContrastAugmentationTransform(), # randomly adjust contrast\n    MeanStdNormalizationTransform(mean\u003d[0.5], std\u003d[0.5])]) \n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "With these transformations we can now wrap our datasets into datamanagers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "from delira.data_loading import DataManager, SequentialSampler, RandomSampler\n",
        "\n",
        "manager_train \u003d DataManager(dataset_train, params.nested_get(\"batch_size\"),\n",
        "                                transforms\u003dtransforms,\n",
        "                                sampler_cls\u003dRandomSampler,\n",
        "                                n_process_augmentation\u003d4)\n",
        "\n",
        "manager_val \u003d DataManager(dataset_val, params.nested_get(\"batch_size\"),\n",
        "                              transforms\u003dtransforms,\n",
        "                              sampler_cls\u003dSequentialSampler,\n",
        "                              n_process_augmentation\u003d4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Model\n",
        "\n",
        "After we have done that, we can specify our model: We will use a smaller version of a [VGG11](https://arxiv.org/pdf/1409.1556.pdf) in this case. We will use more convolutions to reduce the feature dimensionality and reduce the number of units in the linear layers to save up memory (and we only have to deal with 10 classes, not the 1000 imagenet classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from delira.models import AbstractTorchScriptNetwork\n",
        "import torch\n",
        "\n",
        "class Flatten(torch.nn.Module):\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class VGG11TorchScript(AbstractTorchScriptNetwork):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.model \u003d torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels, 64, 3, padding\u003d1), # 32 x 32\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(2), # 16 x 16\n",
        "            torch.nn.Conv2d(64, 128, 3, padding\u003d1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(2), # 8 x 8\n",
        "            torch.nn.Conv2d(128, 256, 3, padding\u003d1), # 4 x 4\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(2), # 4 x 4\n",
        "            torch.nn.Conv2d(256, 512, 3, padding\u003d1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(), # 2 x 2\n",
        "            torch.nn.Conv2d(512, 512, 3, padding\u003d1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(), # 1 x 1\n",
        "            Flatten(),\n",
        "            torch.nn.Linear(1*1*512, num_classes),\n",
        "        )\n",
        "        \n",
        "    @torch.jit.script_method    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return {\"pred\": self.model(x)}\n",
        "    \n",
        "    @staticmethod\n",
        "    def prepare_batch(data_dict, input_device, output_device):\n",
        "        return_dict \u003d {\"data\": torch.from_numpy(batch[\"data\"]).to(\n",
        "            input_device).to(torch.float)}\n",
        "\n",
        "        for key, vals in batch.items():\n",
        "            if key \u003d\u003d \"data\": \n",
        "                continue\n",
        "            return_dict[key] \u003d torch.from_numpy(vals).to(output_device).to(\n",
        "                torch.float)\n",
        "\n",
        "        return return_dict\n",
        "    \n",
        "    @staticmethod\n",
        "    def closure(model, data_dict: dict, optimizers: dict, losses: dict,\n",
        "                fold\u003d0, **kwargs):\n",
        "\n",
        "        loss_vals \u003d {}\n",
        "        total_loss \u003d 0\n",
        "\n",
        "\n",
        "        # predict\n",
        "        inputs \u003d data_dict[\"data\"]\n",
        "        preds \u003d model(inputs)\n",
        "\n",
        "        # calculate losses\n",
        "        for key, crit_fn in losses.items():\n",
        "            _loss_val \u003d crit_fn(preds[\"pred\"], data_dict[\"label\"])\n",
        "            loss_vals[key] \u003d _loss_val.item()\n",
        "            total_loss +\u003d _loss_val\n",
        "\n",
        "        optimizers[\u0027default\u0027].zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizers[\u0027default\u0027].step()\n",
        "\n",
        "        return loss_vals, {k: v.detach()\n",
        "                                for k, v in preds.items()}\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "So let\u0027s evisit, what we have just done.\n",
        "\n",
        "In `delira` all networks must be derived from `delira.models.AbstractNetwork`. For each backend there is a class derived from this class, handling some backend-specific function calls and registrations. For the `TorchScript` Backend this class is `AbstractTorchScriptNetwork` and all TorchScript Networks should be derived from it.\n",
        "\n",
        "\u003e **Note:** This is different from `PyTorch`, where the base class has to be `AbstractPyTorchNetwork`\n",
        "\n",
        "First we defined the network itself (this is the part simply concatenating the layers into a sequential model). Next, we defined the logic to apply, when we want to predict from the model (this is the `forward` method).\n",
        "\n",
        "\u003e **Note:** In `TorchScript` all methods adding options to the computation graph must be decorated with `torch.jit.script_method`. See [here](https://pytorch.org/docs/stable/jit.html#creating-torchscript-code) for more details\n",
        "\n",
        "So far this was plain `TorchScript`. The `prepare_batch` function is not plain TorchScript anymore, but allows us to ensure the data is in the correct shape, has the correct data-type and lies on the correct device. The function above is the standard `prepare_batch` function, which is also implemented in the `AbstractTorchScriptNetwork` and just re-implemented here for the sake of completeness.\n",
        "\n",
        "Same goes for the `closure` function. This function defines the update rule for our parameters (and how to calculate the losses). These funcitons are good to go for many simple networks but can be overwritten for customization when training more complex networks.\n",
        "\n",
        "## Training\n",
        "Now that we have defined our network, we can finally specify our experiment and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning) # ignore UserWarnings raised by dependency code\n",
        "warnings.simplefilter(\"ignore\", FutureWarning) # ignore FutureWarnings raised by dependency code\n",
        "\n",
        "\n",
        "from delira.training import TorchScriptExperiment\n",
        "from delira.training.train_utils import create_optims_default_pytorch\n",
        "\n",
        "if logger is not None:\n",
        "    logger.info(\"Init Experiment\")\n",
        "experiment \u003d TorchScriptExperiment(params, SmallTorchScript,\n",
        "                                   name\u003d\"ClassificationExample\",\n",
        "                                   save_path\u003d\"./tmp/delira_Experiments\",\n",
        "                                   optim_builder\u003dcreate_optims_default_pytorch,\n",
        "                                   key_mapping\u003d{\"x\": \"data\"}\n",
        "                                   gpu_ids\u003d[0])\n",
        "experiment.save()\n",
        "\n",
        "model \u003d experiment.run(manager_train, manager_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Congratulations, you have now trained your first Classification Model using `delira`, we will now predict a few samples from the testset to show, that the networks predictions are valid:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm # utility for progress bars\n",
        "\n",
        "device \u003d torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # set device (use GPU if available)\n",
        "model \u003d model.to(device) # push model to device\n",
        "preds, labels \u003d [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(len(dataset_val))):\n",
        "        img \u003d dataset_val[i][\"data\"] # get image from current batch\n",
        "        img_tensor \u003d torch.from_numpy(img).unsqueeze(0).to(device).to(torch.float) # create a tensor from image, push it to device and add batch dimension\n",
        "        pred_tensor \u003d model(img_tensor) # feed it through the network\n",
        "        pred \u003d pred_tensor.argmax(1).item() # get index with maximum class confidence\n",
        "        label \u003d np.asscalar(dataset_val[i][\"label\"]) # get label from batch\n",
        "        if i % 1000 \u003d\u003d 0:\n",
        "            print(\"Prediction: %d \\t label: %d\" % (pred, label)) # print result\n",
        "        preds.append(pred)\n",
        "        labels.append(label)\n",
        "        \n",
        "# calculate accuracy\n",
        "accuracy \u003d (np.asarray(preds) \u003d\u003d np.asarray(labels)).sum() / len(preds)\n",
        "print(\"Accuracy: %.3f\" % accuracy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}