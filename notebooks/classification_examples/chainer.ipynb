{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Classification with Delira and Chainer - A very short introduction\n",
    "*Author: Justus Schock* \n",
    "\n",
    "*Date: 31.07.2019*\n",
    "\n",
    "This Example shows how to set up a basic classification model and experiment using Chainer.\n",
    "\n",
    "Let's first setup the essential hyperparameters. We will use `delira`'s `Parameters`-class for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\pywt\\_utils.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\google\\protobuf\\descriptor.py:47: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from google.protobuf.pyext import _message\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  _pywrap_tensorflow.RegisterType(\"Mapping\", _collections.Mapping)\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:1287: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  _pywrap_tensorflow.RegisterType(\"Sequence\", _collections.Sequence)\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class ObjectIdentityDictionary(collections.MutableMapping):\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\object_identity.py:112: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class ObjectIdentitySet(collections.MutableSet):\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\jsc7rng\\appdata\\local\\conda\\conda\\envs\\delira-dev\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0731 14:01:15.852783 27416 deprecation_wrapper.py:119] From c:\\users\\jsc7rng\\downloads\\delira\\delira\\models\\backends\\tf_eager\\abstract_network.py:113: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0731 14:01:15.869738 27416 deprecation_wrapper.py:119] From c:\\users\\jsc7rng\\downloads\\delira\\delira\\models\\backends\\tf_graph\\abstract_network.py:20: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger = None\n",
    "import chainer\n",
    "from delira.training import Parameters\n",
    "params = Parameters(fixed_params={\n",
    "    \"model\": {\n",
    "        \"in_channels\": 1, \n",
    "        \"n_outputs\": 10\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 64, # batchsize to use\n",
    "        \"num_epochs\": 10, # number of epochs to train\n",
    "        \"optimizer_cls\": chainer.optimizers.Adam, # optimization algorithm to use\n",
    "        \"optimizer_params\": {'lr': 1e-3}, # initialization parameters for this algorithm\n",
    "        \"losses\": {\"L1\": chainer.functions.mean_absolute_error}, # the loss function\n",
    "        \"lr_sched_cls\": None,  # the learning rate scheduling algorithm to use\n",
    "        \"lr_sched_params\": {}, # the corresponding initialization parameters\n",
    "        \"metrics\": {} # and some evaluation metrics\n",
    "    }\n",
    "}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Since we did not specify any metric, only the `CrossEntropyLoss` will be calculated for each batch. Since we have a classification task, this should be sufficient. We will train our network with a batchsize of 64 by using `Adam` as optimizer of choice.\n",
    "\n",
    "## Logging and Visualization\n",
    "To get a visualization of our results, we should monitor them somehow. For logging we will use `Tensorboard`. Per default the logging directory will be the same as our experiment directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "## Data Preparation\n",
    "### Loading\n",
    "Next we will create some fake data. For this we use the `ClassificationFakeData`-Dataset, which is already implemented in `deliravision`. To avoid getting the exact same data from both datasets, we use a random offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from deliravision.data.fakedata import ClassificationFakeData\n",
    "dataset_train = ClassificationFakeData(num_samples=10000, \n",
    "                                       img_size=(3, 32, 32), \n",
    "                                       num_classes=10)\n",
    "dataset_val = ClassificationFakeData(num_samples=1000, \n",
    "                                     img_size=(3, 32, 32), \n",
    "                                     num_classes=10,\n",
    "                                     rng_offset=10001\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Augmentation\n",
    "For Data-Augmentation we will apply a few transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from batchgenerators.transforms import RandomCropTransform, \\\n",
    "                                        ContrastAugmentationTransform, Compose\n",
    "from batchgenerators.transforms.spatial_transforms import ResizeTransform\n",
    "from batchgenerators.transforms.sample_normalization_transforms import MeanStdNormalizationTransform\n",
    "\n",
    "transforms = Compose([\n",
    "    RandomCropTransform(24), # Perform Random Crops of Size 200 x 200 pixels\n",
    "    ResizeTransform(32), # Resample these crops back to 224 x 224 pixels\n",
    "    ContrastAugmentationTransform(), # randomly adjust contrast\n",
    "    MeanStdNormalizationTransform(mean=[0.5], std=[0.5])]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "With these transformations we can now wrap our datasets into datamanagers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from delira.data_loading import BaseDataManager, SequentialSampler, RandomSampler\n",
    "\n",
    "manager_train = BaseDataManager(dataset_train, params.nested_get(\"batch_size\"),\n",
    "                                transforms=transforms,\n",
    "                                sampler_cls=RandomSampler,\n",
    "                                n_process_augmentation=4)\n",
    "\n",
    "manager_val = BaseDataManager(dataset_val, params.nested_get(\"batch_size\"),\n",
    "                              transforms=transforms,\n",
    "                              sampler_cls=SequentialSampler,\n",
    "                              n_process_augmentation=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Model\n",
    "\n",
    "After we have done that, we can specify our model: We will use a smaller version of a [VGG-Network](https://arxiv.org/pdf/1409.1556.pdf) in this case. We will use more convolutions to reduce the feature dimensionality and reduce the number of units in the linear layers to save up memory (and we only have to deal with 10 classes, not the 1000 imagenet classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delira.models import AbstractChainerNetwork\n",
    "import chainer\n",
    "from functools import partial\n",
    "    \n",
    "    \n",
    "class SmallVGGChainer(AbstractChainerNetwork):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = chainer.Sequential(\n",
    "            chainer.links.Convolution2d(in_channels, 64, 3, padding=1), # 28 x 28\n",
    "            chainer.functions.relu,\n",
    "            partial(chainer.functions.max_pooling_2d, ksize=2), # 14 x 14\n",
    "            chainer.links.Convolution2d(64, 128, 3, padding=1),\n",
    "            chainer.functions.relu,\n",
    "            partial(chainer.functions.max_pooling_2d, ksize=2), # 7 x 7\n",
    "            chainer.links.Convolution2d(128, 256, 3), # 6 x 6\n",
    "            chainer.functions.relu,\n",
    "            partial(chainer.functions.max_pooling_2d, ksize=2), # 3 x 3\n",
    "            chainer.links.Convolution2d(256, 512, 3), # 1 x 1\n",
    "            chainer.functions.flatten,\n",
    "            chainer.links.Linear(1*1*512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return {\"pred\": self.model(x)}\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_batch(data_dict, input_device, output_device):\n",
    "        new_batch = {k: chainer.as_variable(v.astype(np.float32))\n",
    "                     for k, v in batch.items()}\n",
    "\n",
    "        for k, v in new_batch.items():\n",
    "            if k == \"data\":\n",
    "                device = input_device\n",
    "            else:\n",
    "                device = output_device\n",
    "\n",
    "            # makes modification inplace!\n",
    "            v.to_device(device)\n",
    "\n",
    "        return new_batch\n",
    "    \n",
    "    @staticmethod\n",
    "    def closure(model, data_dict: dict, optimizers: dict, losses: dict,\n",
    "                fold=0, **kwargs):\n",
    "\n",
    "        loss_vals = {}\n",
    "        metric_vals = {}\n",
    "        total_loss = 0\n",
    "\n",
    "        inputs = data_dict[\"data\"]\n",
    "        preds = model(inputs)\n",
    "\n",
    "        with chainer.using_config(\"train\", True):\n",
    "            for key, crit_fn in losses.items():\n",
    "                _loss_val = crit_fn(preds[\"pred\"], data_dict[\"label\"])\n",
    "                loss_vals[key] = _loss_val.item()\n",
    "                total_loss += _loss_val\n",
    "\n",
    "        model.cleargrads()\n",
    "        total_loss.backward()\n",
    "        optimizers['default'].update()\n",
    "        \n",
    "        return loss_vals, {k: v.unchain()\n",
    "                           for k, v in preds.items()}\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's evisit, what we have just done.\n",
    "\n",
    "In `delira` all networks must be derived from `delira.models.AbstractNetwork`. For each backend there is a class derived from this class, handling some backend-specific function calls and registrations. For the `Chainer` Backend this class is `AbstractChainerNetwork` and all Chainer Networks should be derived from it.\n",
    "\n",
    "First we defined the network itself (this is the part simply concatenating the layers into a sequential model). Next, we defined the logic to apply, when we want to predict from the model (this is the `forward` method).\n",
    "\n",
    "So far this was plain `Chainer`. The `prepare_batch` function is not plain Chainer anymore, but allows us to ensure the data is in the correct shape, has the correct data-type and lies on the correct device. The function above is the standard `prepare_batch` function, which is also implemented in the `AbstractChainerNetwork` and just re-implemented here for the sake of completeness.\n",
    "\n",
    "Same goes for the `closure` function. This function defines the update rule for our parameters (and how to calculate the losses). These funcitons are good to go for many simple networks but can be overwritten for customization when training more complex networks.\n",
    "\n",
    "\n",
    "## Training\n",
    "Now that we have defined our network, we can finally specify our experiment and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning) # ignore UserWarnings raised by dependency code\n",
    "warnings.simplefilter(\"ignore\", FutureWarning) # ignore FutureWarnings raised by dependency code\n",
    "\n",
    "\n",
    "from delira.training import ChainerExperiment\n",
    "\n",
    "if logger is not None:\n",
    "    logger.info(\"Init Experiment\")\n",
    "experiment = PyTorchExperiment(params, SmallVGGChainer,\n",
    "                               name=\"ClassificationExample\",\n",
    "                               save_path=\"./tmp/delira_Experiments\",\n",
    "                               key_mapping={\"x\": \"data\"}\n",
    "                               gpu_ids=[0])\n",
    "experiment.save()\n",
    "\n",
    "model = experiment.run(manager_train, manager_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Congratulations, you have now trained your first Classification Model using `delira`, we will now predict a few samples from the testset to show, that the networks predictions are valid (for now, this is done manually, but we also have a `Predictor` class to automate stuff like this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm # utility for progress bars\n",
    "\n",
    "device = \"@numpy\"\n",
    "model = model.to(device) # push model to device\n",
    "preds, labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(dataset_val))):\n",
    "        img = dataset_val[i][\"data\"] # get image from current batch\n",
    "        img_tensor = torch.from_numpy(img).unsqueeze(0).to(device).to(torch.float) # create a tensor from image, push it to device and add batch dimension\n",
    "        pred_tensor = model(img_tensor) # feed it through the network\n",
    "        pred = pred_tensor.argmax(1).item() # get index with maximum class confidence\n",
    "        label = np.asscalar(dataset_val[i][\"label\"]) # get label from batch\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Prediction: %d \\t label: %d\" % (pred, label)) # print result\n",
    "        preds.append(pred)\n",
    "        labels.append(label)\n",
    "        \n",
    "# calculate accuracy\n",
    "accuracy = (np.asarray(preds) == np.asarray(labels)).sum() / len(preds)\n",
    "print(\"Accuracy: %.3f\" % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
